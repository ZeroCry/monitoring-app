apiVersion: v1
kind: ConfigMap
metadata:
  name: kapacitor-alerts
  namespace: kube-system
data:
  high_cpu.tick: |
    stream
        |from()
            .measurement('cpu/node_utilization')
            .groupBy('nodename')
        |window()
            .period(5m)
            .every(5m)
        |mean('value').as('used')
        |alert()
            .message('{{ .Level}}: {{ .Name }}/{{ index .Tags "nodename" }} has high cpu usage: {{ index .Fields "used" }}%')
            .warn(lambda: "used" > 0.70)
            .crit(lambda: "used" > 0.80)
            .email()
  high_memory.tick: |
    stream
        |from()
            .measurement('memory/node_utilization')
            .groupBy('nodename')
        |window()
            .period(5m)
            .every(5m)
        |mean('value').as('used')
        |alert()
            .message('{{ .Level}}: {{ .Name }}/{{ index .Tags "nodename" }} has high memory usage: {{ index .Fields "used" }}%')
            .warn(lambda: "used" > 0.70)
            .crit(lambda: "used" > 0.80)
            .email()
  etcd.tick: |
    var period = 30s
    var every = 30s
    var critReset = lambda: "gauge" == 1

    var data = stream
        |from()
            .measurement('etcd_up')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var trigger = data
        |alert()
            .message('{{ .Level}} / ETCD: instance is down: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly(1h)
            .email()

    var data_etcd_health = stream
        |from()
             .measurement('etcd_health')
        |window()
             .period(period)
             .every(every)
             .align()
        |default()
             .field('gauge', -1)

    var etcd_health_deadman = data_etcd_health
        |deadman(0.0, 3m)
             .message('ETCD cluster is {{ if eq .Level "OK" }}alive{{ else }}unhealthy{{ end }}')
             .email()

    var trigger_etcd_health = data_etcd_health
        |alert()
             .message('{{ .Level}} / ETCD: cluster is unhealthy')
             .crit(lambda: "gauge" == 0)
             .stateChangesOnly(30m)
             .critReset(critReset)
             .email()

    //
    // latency between nodes
    var info = 0.5 // s
    var warn = 1   // s
    var crit = 2   // s

    var data_etcd_latency = stream
        |from()
            .measurement('etcd_followers_latency')
            .groupBy('followerName')
        |window()
            .period(period)
            .every(every)
            .align()
        |mean('gauge')
            .as('stat')

    var trigger_etcd_latency = data_etcd_latency
        |alert()
            .message('{{ .Level}} / ETCD: High latency between leader and follower {{ index .Tags "followerName" }}: {{ index .Fields "stat" }}')
            .info(lambda: "stat" > info)
            .warn(lambda: "stat" > warn)
            .crit(lambda: "stat" > crit)
            .email()

    //
    // rate of errors of RAFT requests
    var infoRate = 10
    var warnRate = 15
    var critRate = 25
    var data_etcd_raft_fail = stream
        |from()
            .measurement('etcd_followers_raft_fail')
            .groupBy('followerName')
        |window()
            .period(period)
            .every(every)
            .align()
        |derivative('gauge')
            .as('rate')
            .unit(10s)
            .nonNegative()

    var trigger_etcd_raft_fail = data_etcd_raft_fail
        |alert()
            .message('{{ .Level}} / ETCD: High rate of failed RAFT requests between leader and follower {{ index .Tags "followerName" }}: {{ index .Fields "rate"}}')
            .info(lambda: "rate" > infoRate)
            .warn(lambda: "rate" > warnRate)
            .crit(lambda: "rate" > critRate)
            .email()
