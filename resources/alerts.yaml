apiVersion: v1
kind: ConfigMap
metadata:
  name: kapacitor-alerts
  namespace: kube-system
data:
  high_cpu.tick: |
    stream
        |from()
            .measurement('cpu/node_utilization')
            .groupBy('nodename')
        |window()
            .period(5m)
            .every(5m)
        |mean('value').as('used')
        |alert()
            .message('{{ .Level}}: {{ .Name }}/{{ index .Tags "nodename" }} has high cpu usage: {{ index .Fields "used" }}%')
            .warn(lambda: "used" > 0.70)
            .crit(lambda: "used" > 0.80)
            .email()
  high_memory.tick: |
    stream
        |from()
            .measurement('memory/node_utilization')
            .groupBy('nodename')
        |window()
            .period(5m)
            .every(5m)
        |mean('value').as('used')
        |alert()
            .message('{{ .Level}}: {{ .Name }}/{{ index .Tags "nodename" }} has high memory usage: {{ index .Fields "used" }}%')
            .warn(lambda: "used" > 0.70)
            .crit(lambda: "used" > 0.80)
            .email()
  etcd.tick: |
    var period = 1m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_etcd_up = stream
        |from()
            .measurement('etcd_up')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var trigger_etcd_up = data_etcd_up
        |alert()
            .message('{{ .Level }} / ETCD: instance is down: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly(1h)
            .details('''
    <b>{{ .Message }}</b>
    Level: {{ .Level }}
    Nodename: {{ index .Tags "nodename" }}
    ''')
            .email()


    var data_etcd_health = stream
        |from()
            .measurement('etcd_health')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var etcd_health_deadman = data_etcd_health
        |deadman(0.0, 3m)
            .message('ETCD cluster is {{ if eq .Level "OK" }}alive{{ else }}unhealthy{{ end }}')
            .email()

    var trigger_etcd_health = data_etcd_health
        |alert()
            .message('{{ .Level }} / ETCD: cluster is unhealthy')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly(30m)
            .details('''
    <b>{{ .Message }}</b>
    Level: {{ .Level }}
    ''')
            .email()

    var infoRate = 10
    var warnRate = 15
    var critRate = 25
    var infoRateReset = 0
    var warnRateReset = 10
    var critRateReset = 15
    var data_etcd_raft_fail = stream
        |from()
            .measurement('etcd_followers_raft_fail')
            .groupBy('followerName')
        |window()
            .period(period)
            .every(every)
            .align()
        |derivative('gauge')
            .as('rate')
            .unit(10s)
            .nonNegative()

    var trigger_etcd_raft_fail = data_etcd_raft_fail
        |alert()
            .message('{{ .Level }} / ETCD: High rate of failed RAFT requests between leader and follower {{ index .Tags "followerName" }}')
            .info(lambda: "rate" > infoRate)
            .infoReset(lambda: "rate" < infoRateReset)
            .warn(lambda: "rate" > warnRate)
            .warnReset(lambda: "rate" < warnRateReset)
            .crit(lambda: "rate" > critRate)
            .critReset(lambda: "rate" < critRateReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    Level: {{ .Level }}
    Nodename: {{ index .Tags "nodename" }}
    Number of failed requests in last minute: {{ index .Fields "rate"  | printf "%0.0f" }}
        ''')
            .email()

  etcd_latency_batch.tick: |
    var period = 5m
    var every = 1m
    var data_etcd_latency = batch
        |query('''SELECT (DERIVATIVE(count,1m) * 0.95) AS count, DERIVATIVE("0.512",1m) AS v512, DERIVATIVE("1.024",1m) AS v1024 FROM "k8s"."default"."etcd_rafthttp_message_sent_latency_seconds" WHERE "msgType" = 'MsgHeartbeat' AND "sendingType" = 'message' ''')
            .period(period)
            .every(every)
            .groupBy('remoteID')

    var count = data_etcd_latency
        |mean('count')
    var v512 = data_etcd_latency
        |mean('v512')
    var v1024 = data_etcd_latency
        |mean('v1024')

    var trigger_etcd_latency = count
        |join(v512,v1024)
            .as('count', 'v512', 'v1024')
            .tolerance(10s)

    trigger_etcd_latency
        |alert()
            .message('{{ .Level }} / ETCD: High latency between leader and follower {{ index .Tags "followerName" }}')
            .warn(lambda: "count.mean" > "v512.mean")
            .crit(lambda: "count.mean" > "v1024.mean")
            .details('''
    <b>{{ .Message }}</b>
    Level: {{ .Level }}
    ETCD instance: {{ index .Tags "followerName" }}
    Latency greater than: {{ if eq .Level "WARNING" }}512{{ else }}1024{{ end }}ms
    ''')
            .email()
